{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzmVxdQyOg6fWrab6sFsMu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaveenTark/Reinforcement-Learning-with-CPP-and-Python/blob/main/Classical_RL/PolicyIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fT1n61cSw3Y-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "# Define Environment\n",
        "class GridworldMDP:\n",
        "    def __init__(self, grid, start, goal, traps, obstacles, p_success=0.8, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Initialize Gridworld MDP.\n",
        "\n",
        "        :param grid: (int, int) Grid size (rows, cols)\n",
        "        :param start: (int, int) Starting position\n",
        "        :param goal: (int, int) Goal position\n",
        "        :param traps: List of (int, int) positions with trap rewards (-1)\n",
        "        :param obstacles: List of (int, int) impassable positions\n",
        "        :param p_success: Probability of moving in the intended direction\n",
        "        :param gamma: Discount factor for future rewards\n",
        "        \"\"\"\n",
        "        self.grid_size = grid\n",
        "\n",
        "        # Validate initializations\n",
        "        assert start != goal, \"Start and goal positions cannot be the same.\"\n",
        "        assert 0 <= start[0] < grid[0] and 0 <= start[1] < grid[1], \"Invalid start position.\"\n",
        "        assert 0 <= goal[0] < grid[0] and 0 <= goal[1] < grid[1], \"Invalid goal position.\"\n",
        "        assert all(0 <= trap[0] < grid[0] and 0 <= trap[1] < grid[1] for trap in traps), \"Invalid trap position.\"\n",
        "        assert all(0 <= obs[0] < grid[0] and 0 <= obs[1] < grid[1] for obs in obstacles), \"Invalid obstacle position.\"\n",
        "        assert 0 <= p_success <= 1, \"Invalid success probability.\"\n",
        "        assert 0 <= gamma <= 1, \"Invalid discount factor.\"\n",
        "\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.traps = set(traps)\n",
        "        self.obstacles = set(obstacles)\n",
        "        self.p_success = p_success\n",
        "        self.gamma = gamma\n",
        "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # UP, DOWN, LEFT, RIGHT\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the agent to the start position.\"\"\"\n",
        "        self.agent_pos = self.start\n",
        "        return self.agent_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment.\n",
        "\n",
        "        :param action: (int) Action index [0=UP, 1=DOWN, 2=LEFT, 3=RIGHT]\n",
        "        :return: (new_state, reward, done)\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.p_success:\n",
        "            move = self.actions[action]  # Intended move\n",
        "        else:\n",
        "            move = self.actions[np.random.choice(4)]  # Random move\n",
        "\n",
        "        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])\n",
        "\n",
        "        # Check for out-of-bounds or obstacle\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size[0] or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size[1] or\n",
        "            new_pos in self.obstacles):\n",
        "            new_pos = self.agent_pos  # Stay in place\n",
        "\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Assign rewards\n",
        "        if new_pos == self.goal:\n",
        "            return new_pos, 1.0, True  # Goal reached\n",
        "        elif new_pos in self.traps:\n",
        "            return new_pos, -1.0, True  # Trap\n",
        "        else:\n",
        "            return new_pos, 0.0, False  # Neutral move\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "      \"\"\"\n",
        "      Return a list of transitions for a given state and action.\n",
        "      Each transition is a tuple: (next_state, reward, done, probability)\n",
        "      This function computes all possible outcomes given the stochastic dynamics.\n",
        "      \"\"\"\n",
        "      outcomes = {}\n",
        "      # Intended move\n",
        "      intended_move = self.actions[action]\n",
        "      next_state = (state[0] + intended_move[0], state[1] + intended_move[1])\n",
        "      if (next_state[0] < 0 or next_state[0] >= self.grid_size[0] or\n",
        "          next_state[1] < 0 or next_state[1] >= self.grid_size[1] or\n",
        "          next_state in self.obstacles):\n",
        "          next_state = state  # Stay in place if move is invalid\n",
        "      if next_state == self.goal:\n",
        "          reward, done = 1.0, True\n",
        "      elif next_state in self.traps:\n",
        "          reward, done = -1.0, True\n",
        "      else:\n",
        "          reward, done = 0.0, False\n",
        "\n",
        "      outcomes[next_state] = (reward, done, self.p_success)\n",
        "\n",
        "      # Random moves (with probability (1 - p_success) equally divided among all actions)\n",
        "      random_prob = (1 - self.p_success) / 4.0\n",
        "      for rand_action in range(4):\n",
        "          rand_move = self.actions[rand_action]\n",
        "          rand_state = (state[0] + rand_move[0], state[1] + rand_move[1])\n",
        "          if (rand_state[0] < 0 or rand_state[0] >= self.grid_size[0] or\n",
        "              rand_state[1] < 0 or rand_state[1] >= self.grid_size[1] or\n",
        "              rand_state in self.obstacles):\n",
        "              rand_state = state  # Stay in place if invalid\n",
        "          if rand_state == self.goal:\n",
        "              rand_reward, rand_done = 1.0, True\n",
        "          elif rand_state in self.traps:\n",
        "              rand_reward, rand_done = -1.0, True\n",
        "          else:\n",
        "              rand_reward, rand_done = 0.0, False\n",
        "\n",
        "          # If outcome already exists, add probability\n",
        "          if rand_state in outcomes:\n",
        "              prev_reward, prev_done, prev_prob = outcomes[rand_state]\n",
        "              outcomes[rand_state] = (rand_reward, rand_done, prev_prob + random_prob)\n",
        "          else:\n",
        "              outcomes[rand_state] = (rand_reward, rand_done, random_prob)\n",
        "\n",
        "      transitions = []\n",
        "      for ns, (rew, done, prob) in outcomes.items():\n",
        "          transitions.append((ns, rew, done, prob))\n",
        "      return transitions\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\"Returns a list of valid actions from a given state.\"\"\"\n",
        "        actions = []\n",
        "        for i, move in enumerate(self.actions):\n",
        "            new_pos = (state[0] + move[0], state[1] + move[1])\n",
        "            if (0 <= new_pos[0] < self.grid_size[0] and\n",
        "                0 <= new_pos[1] < self.grid_size[1] and\n",
        "                new_pos not in self.obstacles):\n",
        "                actions.append(i)\n",
        "        return actions\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Print the grid with the agent's position.\"\"\"\n",
        "        grid_display = np.full(self.grid_size, \".\", dtype=str)\n",
        "        grid_display[self.goal] = \"G\"\n",
        "        for trap in self.traps:\n",
        "            grid_display[trap] = \"T\"\n",
        "        for obs in self.obstacles:\n",
        "            grid_display[obs] = \"#\"\n",
        "        grid_display[self.agent_pos] = \"A\"  # Agent\n",
        "\n",
        "        for row in grid_display:\n",
        "            print(\" \".join(row))\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridWorld1 = GridworldMDP((4,4), (0,0), (3,3), [(3,2)], [(1,3),(2,1)],1,0.9)"
      ],
      "metadata": {
        "id": "RAz3FMqxxBIR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridWorld1.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBxb7J9sxEAX",
        "outputId": "0be7da04-c73b-47ec-a904-11a349c51497"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A . . .\n",
            ". . . #\n",
            ". # . .\n",
            ". . T G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GridWorld1.get_possible_actions(GridWorld1.agent_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9vqm5sny89z",
        "outputId": "9f08ff01-eeef-4820-ce6a-9d59a96ca13b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Policy Iteration\n",
        "values = np.zeros((4, 4))\n",
        "max_no_iter = 10000\n",
        "threshold = 1e-6\n",
        "policy = {}\n",
        "\n",
        "# Creating initial random policy\n",
        "action_mapping = {0: \"UP\", 1: \"DOWN\", 2: \"LEFT\", 3 : \"RIGHT\"}  # UP, DOWN, LEFT, RIGHT\n",
        "for i in range(values.shape[0]):\n",
        "    for j in range(values.shape[1]):\n",
        "        state = (i, j)\n",
        "        if state in GridWorld1.traps:\n",
        "            policy[state] = \"NAN\"\n",
        "            continue\n",
        "        if state == GridWorld1.goal:\n",
        "            policy[state] = \"NAN\"\n",
        "            continue\n",
        "        if state in GridWorld1.obstacles:\n",
        "            policy[state] = \"NAN\"\n",
        "            continue\n",
        "        else:\n",
        "           actions = GridWorld1.get_possible_actions(state)\n",
        "           policy[state]= random.choice(actions)\n",
        "\n",
        "print(\"Initial policy as grid:\")\n",
        "grid_display = np.full(GridWorld1.grid_size, \".\", dtype=str)\n",
        "for state, action in policy.items():\n",
        "    if action != \"NAN\":  # Skip terminal and obstacle states\n",
        "        grid_display[state] = action_mapping[action]\n",
        "grid_display[GridWorld1.goal] = \"G\"  # Goal\n",
        "for trap in GridWorld1.traps:\n",
        "    grid_display[trap] = \"T\"  # Traps\n",
        "for obs in GridWorld1.obstacles:\n",
        "    grid_display[obs] = \"#\"  # Obstacles\n",
        "for row in grid_display:\n",
        "    print(\" \".join(row))\n",
        "\n",
        "for iter in range(max_no_iter):\n",
        "\n",
        "  # Policy Evaluation:\n",
        "  for k in range(max_no_iter):\n",
        "    values_copy = values.copy()\n",
        "    for i in range(values.shape[0]):\n",
        "        for j in range(values.shape[1]):\n",
        "            state = (i, j)\n",
        "            # Terminal states and obstacles handling:\n",
        "            if state in GridWorld1.traps:\n",
        "                values[i, j] = -1.0\n",
        "                continue\n",
        "            if state == GridWorld1.goal:\n",
        "                values[i, j] = 1.0\n",
        "                continue\n",
        "            if state in GridWorld1.obstacles:\n",
        "                continue\n",
        "\n",
        "\n",
        "            action = policy[state]\n",
        "\n",
        "            action_value = 0\n",
        "            transitions = GridWorld1.get_transitions(state, action)\n",
        "            # Sum over all transitions to compute expected value for this action\n",
        "            for next_state, reward, done, prob in transitions:\n",
        "              # To avoid double counting of reward in terminal state\n",
        "              if done:\n",
        "                action_value += prob * reward\n",
        "              else:\n",
        "                action_value += prob * (reward + GridWorld1.gamma * values[next_state])\n",
        "\n",
        "            values[i, j] = action_value\n",
        "\n",
        "    # Check convergence\n",
        "    if np.all(np.abs(values - values_copy) < threshold):\n",
        "        break\n",
        "  # Policy improvement\n",
        "  policy_copy = policy.copy()\n",
        "  for i in range(values.shape[0]):\n",
        "    for j in range(values.shape[1]):\n",
        "      state = (i, j)\n",
        "      if state in GridWorld1.traps:\n",
        "        policy[state] = \"NAN\"\n",
        "        continue\n",
        "      if state == GridWorld1.goal:\n",
        "        policy[state] = \"NAN\"\n",
        "        continue\n",
        "      if state in GridWorld1.obstacles:\n",
        "        continue\n",
        "      else:\n",
        "        best_action_value = -np.inf\n",
        "        best_action = None\n",
        "        possible_actions = GridWorld1.get_possible_actions(state)\n",
        "        for action in possible_actions:\n",
        "            action_value = 0\n",
        "            transitions = GridWorld1.get_transitions(state, action)\n",
        "            for next_state, reward, done, prob in transitions:\n",
        "                if done:\n",
        "                    action_value += prob * reward\n",
        "                else:\n",
        "                    action_value += prob * (reward + GridWorld1.gamma * values[next_state])\n",
        "            if action_value > best_action_value:\n",
        "                best_action_value = action_value\n",
        "                best_action = action\n",
        "        policy[state] = best_action\n",
        "  if np.all(np.array(list(policy.values())) == np.array(list(policy_copy.values()))):\n",
        "    break\n",
        "\n",
        "for i in policy:\n",
        "  if policy[i] != \"NAN\":\n",
        "    policy[i] = action_mapping[policy[i]]\n",
        "# Render policy as grid\n",
        "print(\"Final policy as grid:\")\n",
        "grid_display = np.full(GridWorld1.grid_size, \".\", dtype=str)\n",
        "for state, action in policy.items():\n",
        "    if action != \"NAN\":  # Skip terminal and obstacle states\n",
        "        grid_display[state] = action\n",
        "grid_display[GridWorld1.goal] = \"G\"  # Goal\n",
        "for trap in GridWorld1.traps:\n",
        "    grid_display[trap] = \"T\"  # Traps\n",
        "for obs in GridWorld1.obstacles:\n",
        "    grid_display[obs] = \"#\"  # Obstacles\n",
        "for row in grid_display:\n",
        "    print(\" \".join(row))\n",
        "print(\"Final state values:\")\n",
        "print(values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUvGwcHvxFeP",
        "outputId": "81e47ebe-7529-445b-a7bf-668037068b7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial policy as grid:\n",
            "R L L L\n",
            "R R U #\n",
            "U # R L\n",
            "R L T G\n",
            "Final policy as grid:\n",
            "D D D L\n",
            "R R D #\n",
            "U # R D\n",
            "U L T G\n",
            "Final state values:\n",
            "[[ 0.59049    0.6561     0.729      0.6561   ]\n",
            " [ 0.6561     0.729      0.81       0.       ]\n",
            " [ 0.59049    0.         0.9        1.       ]\n",
            " [ 0.531441   0.4782969 -1.         1.       ]]\n"
          ]
        }
      ]
    }
  ]
}