{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqZUSeONG6AwLU5DqLtwxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaveenTark/Reinforcement-Learning-with-CPP-and-Python/blob/main/Classical_RL/ValueIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define Environment\n",
        "class GridworldMDP:\n",
        "    def __init__(self, grid, start, goal, traps, obstacles, p_success=0.8, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Initialize Gridworld MDP.\n",
        "\n",
        "        :param grid: (int, int) Grid size (rows, cols)\n",
        "        :param start: (int, int) Starting position\n",
        "        :param goal: (int, int) Goal position\n",
        "        :param traps: List of (int, int) positions with trap rewards (-1)\n",
        "        :param obstacles: List of (int, int) impassable positions\n",
        "        :param p_success: Probability of moving in the intended direction\n",
        "        :param gamma: Discount factor for future rewards\n",
        "        \"\"\"\n",
        "        self.grid_size = grid\n",
        "\n",
        "        # Validate initializations\n",
        "        assert start != goal, \"Start and goal positions cannot be the same.\"\n",
        "        assert 0 <= start[0] < grid[0] and 0 <= start[1] < grid[1], \"Invalid start position.\"\n",
        "        assert 0 <= goal[0] < grid[0] and 0 <= goal[1] < grid[1], \"Invalid goal position.\"\n",
        "        assert all(0 <= trap[0] < grid[0] and 0 <= trap[1] < grid[1] for trap in traps), \"Invalid trap position.\"\n",
        "        assert all(0 <= obs[0] < grid[0] and 0 <= obs[1] < grid[1] for obs in obstacles), \"Invalid obstacle position.\"\n",
        "        assert 0 <= p_success <= 1, \"Invalid success probability.\"\n",
        "        assert 0 <= gamma <= 1, \"Invalid discount factor.\"\n",
        "\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.traps = set(traps)\n",
        "        self.obstacles = set(obstacles)\n",
        "        self.p_success = p_success\n",
        "        self.gamma = gamma\n",
        "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # UP, DOWN, LEFT, RIGHT\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the agent to the start position.\"\"\"\n",
        "        self.agent_pos = self.start\n",
        "        return self.agent_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment.\n",
        "\n",
        "        :param action: (int) Action index [0=UP, 1=DOWN, 2=LEFT, 3=RIGHT]\n",
        "        :return: (new_state, reward, done)\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.p_success:\n",
        "            move = self.actions[action]  # Intended move\n",
        "        else:\n",
        "            move = self.actions[np.random.choice(4)]  # Random move\n",
        "\n",
        "        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])\n",
        "\n",
        "        # Check for out-of-bounds or obstacle\n",
        "        if (new_pos[0] < 0 or new_pos[0] >= self.grid_size[0] or\n",
        "            new_pos[1] < 0 or new_pos[1] >= self.grid_size[1] or\n",
        "            new_pos in self.obstacles):\n",
        "            new_pos = self.agent_pos  # Stay in place\n",
        "\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        # Assign rewards\n",
        "        if new_pos == self.goal:\n",
        "            return new_pos, 1.0, True  # Goal reached\n",
        "        elif new_pos in self.traps:\n",
        "            return new_pos, -1.0, True  # Trap\n",
        "        else:\n",
        "            return new_pos, 0.0, False  # Neutral move\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "      \"\"\"\n",
        "      Return a list of transitions for a given state and action.\n",
        "      Each transition is a tuple: (next_state, reward, done, probability)\n",
        "      This function computes all possible outcomes given the stochastic dynamics.\n",
        "      \"\"\"\n",
        "      outcomes = {}\n",
        "      # Intended move\n",
        "      intended_move = self.actions[action]\n",
        "      next_state = (state[0] + intended_move[0], state[1] + intended_move[1])\n",
        "      if (next_state[0] < 0 or next_state[0] >= self.grid_size[0] or\n",
        "          next_state[1] < 0 or next_state[1] >= self.grid_size[1] or\n",
        "          next_state in self.obstacles):\n",
        "          next_state = state  # Stay in place if move is invalid\n",
        "      if next_state == self.goal:\n",
        "          reward, done = 1.0, True\n",
        "      elif next_state in self.traps:\n",
        "          reward, done = -1.0, True\n",
        "      else:\n",
        "          reward, done = 0.0, False\n",
        "\n",
        "      outcomes[next_state] = (reward, done, self.p_success)\n",
        "\n",
        "      # Random moves (with probability (1 - p_success) equally divided among all actions)\n",
        "      random_prob = (1 - self.p_success) / 4.0\n",
        "      for rand_action in range(4):\n",
        "          rand_move = self.actions[rand_action]\n",
        "          rand_state = (state[0] + rand_move[0], state[1] + rand_move[1])\n",
        "          if (rand_state[0] < 0 or rand_state[0] >= self.grid_size[0] or\n",
        "              rand_state[1] < 0 or rand_state[1] >= self.grid_size[1] or\n",
        "              rand_state in self.obstacles):\n",
        "              rand_state = state  # Stay in place if invalid\n",
        "          if rand_state == self.goal:\n",
        "              rand_reward, rand_done = 1.0, True\n",
        "          elif rand_state in self.traps:\n",
        "              rand_reward, rand_done = -1.0, True\n",
        "          else:\n",
        "              rand_reward, rand_done = 0.0, False\n",
        "\n",
        "          # If outcome already exists, add probability\n",
        "          if rand_state in outcomes:\n",
        "              prev_reward, prev_done, prev_prob = outcomes[rand_state]\n",
        "              outcomes[rand_state] = (rand_reward, rand_done, prev_prob + random_prob)\n",
        "          else:\n",
        "              outcomes[rand_state] = (rand_reward, rand_done, random_prob)\n",
        "\n",
        "      transitions = []\n",
        "      for ns, (rew, done, prob) in outcomes.items():\n",
        "          transitions.append((ns, rew, done, prob))\n",
        "      return transitions\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\"Returns a list of valid actions from a given state.\"\"\"\n",
        "        actions = []\n",
        "        for i, move in enumerate(self.actions):\n",
        "            new_pos = (state[0] + move[0], state[1] + move[1])\n",
        "            if (0 <= new_pos[0] < self.grid_size[0] and\n",
        "                0 <= new_pos[1] < self.grid_size[1] and\n",
        "                new_pos not in self.obstacles):\n",
        "                actions.append(i)\n",
        "        return actions\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Print the grid with the agent's position.\"\"\"\n",
        "        grid_display = np.full(self.grid_size, \".\", dtype=str)\n",
        "        grid_display[self.goal] = \"G\"\n",
        "        for trap in self.traps:\n",
        "            grid_display[trap] = \"T\"\n",
        "        for obs in self.obstacles:\n",
        "            grid_display[obs] = \"#\"\n",
        "        grid_display[self.agent_pos] = \"A\"  # Agent\n",
        "\n",
        "        for row in grid_display:\n",
        "            print(\" \".join(row))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "iRcv8QJ_j0-R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ezhUc7Cjr-KM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GridWorld1 = GridworldMDP((4,4), (0,0), (3,3), [(3,2)], [(1,3),(2,1)],1,0.9)"
      ],
      "metadata": {
        "id": "pJK6K6zNkRTv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridWorld1.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqqtQZrBsH7r",
        "outputId": "2d73cd89-1da1-40e1-e611-103bf98c6142"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A . . .\n",
            ". . . #\n",
            ". # . .\n",
            ". . T G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(GridWorld1.agent_pos)\n",
        "GridWorld1.get_transitions(GridWorld1.agent_pos, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPlv-qeZ82w3",
        "outputId": "cf370c36-e271-4f2a-a402-e434e0873977"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((0, 0), 0.0, False, 1.0),\n",
              " ((1, 0), 0.0, False, 0.0),\n",
              " ((0, 1), 0.0, False, 0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Value iteration implementation\n",
        "values = np.zeros((4, 4))\n",
        "max_no_iter = 10000\n",
        "threshold = 1e-6\n",
        "for k in range(max_no_iter):\n",
        "    copy = values.copy()\n",
        "    for i in range(values.shape[0]):\n",
        "        for j in range(values.shape[1]):\n",
        "            state = (i, j)\n",
        "            # Terminal states and obstacles handling:\n",
        "            if state in GridWorld1.traps:\n",
        "                values[i, j] = -1.0\n",
        "                continue\n",
        "            if state == GridWorld1.goal:\n",
        "                values[i, j] = 1.0\n",
        "                continue\n",
        "            if state in GridWorld1.obstacles:\n",
        "                continue\n",
        "\n",
        "            best_action_value = -np.inf\n",
        "            possible_actions = GridWorld1.get_possible_actions(state)\n",
        "            for action in possible_actions:\n",
        "                action_value = 0\n",
        "                transitions = GridWorld1.get_transitions(state, action)\n",
        "                # Sum over all transitions to compute expected value for this action\n",
        "                for next_state, reward, done, prob in transitions:\n",
        "                  # To avoid double counting of reward in terminal state\n",
        "                  if done:\n",
        "                    action_value += prob * reward\n",
        "                  else:\n",
        "                    action_value += prob * (reward + GridWorld1.gamma * values[next_state])\n",
        "                best_action_value = max(best_action_value, action_value)\n",
        "            values[i, j] = best_action_value\n",
        "\n",
        "    # Check convergence\n",
        "    if np.all(np.abs(values - copy) < threshold):\n",
        "        break\n",
        "\n",
        "print(\"Converged in {} iterations\".format(k + 1))\n",
        "print(\"Final state values:\")\n",
        "print(values)\n",
        "\n",
        "\n",
        "\n",
        "# Policy\n",
        "policy = {}\n",
        "action_mapping = {0: \"UP\", 1: \"DOWN\", 2: \"LEFT\", 3 : \"RIGHT\"}  # UP, DOWN, LEFT, RIGHT\n",
        "for i in range(values.shape[0]):\n",
        "    for j in range(values.shape[1]):\n",
        "        state = (i, j)\n",
        "        if state in GridWorld1.traps:\n",
        "            policy[state] = \"NAN\"\n",
        "            continue\n",
        "        if state == GridWorld1.goal:\n",
        "            policy[state] = \"NAN\"\n",
        "            continue\n",
        "        if state in GridWorld1.obstacles:\n",
        "            policy[state] = \"NAN\"\n",
        "            continue\n",
        "        best_action_value = -np.inf\n",
        "        best_action = None\n",
        "        possible_actions = GridWorld1.get_possible_actions(state)\n",
        "        for action in possible_actions:\n",
        "            action_value = 0\n",
        "            transitions = GridWorld1.get_transitions(state, action)\n",
        "            for next_state, reward, done, prob in transitions:\n",
        "                if done:\n",
        "                    action_value += prob * reward\n",
        "                else:\n",
        "                    action_value += prob * (reward + GridWorld1.gamma * values[next_state])\n",
        "            if action_value > best_action_value:\n",
        "                best_action_value = action_value\n",
        "                best_action = action\n",
        "        policy[state] = action_mapping[best_action]\n",
        "\n",
        "print(\"Final policy:\")\n",
        "print(policy)\n",
        "# Render policy as grid\n",
        "print(\"Final policy as grid:\")\n",
        "grid_display = np.full(GridWorld1.grid_size, \".\", dtype=str)\n",
        "for state, action in policy.items():\n",
        "    if action != \"NAN\":  # Skip terminal and obstacle states\n",
        "        grid_display[state] = action\n",
        "grid_display[GridWorld1.goal] = \"G\"  # Goal\n",
        "for trap in GridWorld1.traps:\n",
        "    grid_display[trap] = \"T\"  # Traps\n",
        "for obs in GridWorld1.obstacles:\n",
        "    grid_display[obs] = \"#\"  # Obstacles\n",
        "for row in grid_display:\n",
        "    print(\" \".join(row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkQ2PDhJ6DxW",
        "outputId": "d95c8fea-ec3c-46d3-f03c-3c9def573fca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 7 iterations\n",
            "Final state values:\n",
            "[[ 0.59049    0.6561     0.729      0.6561   ]\n",
            " [ 0.6561     0.729      0.81       0.       ]\n",
            " [ 0.59049    0.         0.9        1.       ]\n",
            " [ 0.531441   0.4782969 -1.         1.       ]]\n",
            "Final policy:\n",
            "{(0, 0): 'DOWN', (0, 1): 'DOWN', (0, 2): 'DOWN', (0, 3): 'LEFT', (1, 0): 'RIGHT', (1, 1): 'RIGHT', (1, 2): 'DOWN', (1, 3): 'NAN', (2, 0): 'UP', (2, 1): 'NAN', (2, 2): 'RIGHT', (2, 3): 'DOWN', (3, 0): 'UP', (3, 1): 'LEFT', (3, 2): 'NAN', (3, 3): 'NAN'}\n",
            "Final policy as grid:\n",
            "D D D L\n",
            "R R D #\n",
            "U # R D\n",
            "U L T G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q) Do we need to train an agent for each configuration of gridworld? Isn't that not scalable?   \n",
        "Ans) PPO with randomised training can possibly help\n",
        "\n",
        "Q) Can we train an agent to tackle multiple gridsizes with different configuration?\n",
        "\n",
        "Q) Can we use features from agent trained on one or multiple grid size for an unknown gridsize?"
      ],
      "metadata": {
        "id": "DPQ1n3tRrTZc"
      }
    }
  ]
}